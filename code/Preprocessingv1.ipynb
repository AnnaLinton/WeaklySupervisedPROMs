{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e7d896a",
   "metadata": {},
   "source": [
    "# Preprocessing on short text \n",
    "\n",
    "This notebook contains text to preprocess short text \n",
    "It will perform:\n",
    "* expand contractions\n",
    "* Correct spelling errors - Pyspellchecker to spell check\n",
    "* Remove punctuation - NLTK \n",
    "* Lower case - NLTK\n",
    "* Remove numbers - NLTK\n",
    "* Remove stop words - NLTK\n",
    "* Lemmatisation - NLTK Wordnet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fba51ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c6c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a-lin\\anaconda3\\envs\\phdproject\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from wtc_functions import load_ftc_data, expanding, clean_text\n",
    "import wtc_functions as wtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2320f497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a-lin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# nltk.download(\"corpus\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59d56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words(\"english\")             \n",
    "lemmatiser = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1363ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fe392ff",
   "metadata": {},
   "source": [
    "# Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d374213",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"datapath\" #excel file \n",
    "datadf = load_ftc_data(datafile)\n",
    "datadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load PCa data\n",
    "datafile = \"datapath\" #excel file \n",
    "\n",
    "data = pd.read_excel(datafile, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe where each row is a comment. \n",
    "# The 1st column is pateint ID, 2nd column is the free text comment and 3rd column is question number.\n",
    "\n",
    "comments = []\n",
    "indexes = []\n",
    "questions = []\n",
    "\n",
    "for i in range(7):\n",
    "    col = data.columns[i+1]\n",
    "    col_data = data[col].dropna()\n",
    "    comments.extend(col_data)\n",
    "    indexes.extend(data.loc[col_data.index, \"Supplied Member Number\"])\n",
    "    questions.extend([str(i+1)] * len(col_data))\n",
    "\n",
    "commentsdf = pd.DataFrame({'Patient ID': indexes, 'Comments': comments, 'Question': questions})\n",
    "commentsdf = commentsdf[['Patient ID', 'Comments', 'Question']]\n",
    "\n",
    "commentsdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44df8669",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Adds processed comemnts as a new row - to look at the effect of each processing step on the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7dfcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf = commentsdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957f6bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Expand contractions    \n",
    "datadf['expanded_contractions'] = wtc.expanding(datadf['Comments'])\n",
    "datadf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e6eecb7",
   "metadata": {},
   "source": [
    "## Remove punctuations, numbers, lowercase and lemmatise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fd0974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "sp = SpellCorrector(corpus=\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "def reg_words(comment):\n",
    "    # replace the confidentiality filtersto maintain parts of speech tagging\n",
    "\n",
    "    src1_str = re.compile(\"address removed\", re.IGNORECASE)\n",
    "    src2_str = re.compile(\"name removed\", re.IGNORECASE)\n",
    "    src3_str = re.compile(\"G.P\", re.IGNORECASE)\n",
    "    out = src1_str.sub(\"address_removed\", comment)\n",
    "    out = src2_str.sub(\"name_removed\", out)\n",
    "    out = src2_str.sub(\"GP\", out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")            \n",
    "lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# sp = SpellCorrector(corpus=\"english\") \n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return \"\"  # for easy if-statement\n",
    "    \n",
    "def token_text(comment):\n",
    "    \"\"\"input: list of comments\n",
    "    output: list of text lemmatised and tokenised\"\"\"\n",
    "    tok_text = []\n",
    "    token_tag = nltk.pos_tag(comment)  # tags words with POS tag\n",
    "\n",
    "    for token, tag in token_tag:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "#         print (wntag)\n",
    "        if token == \"nhs\":\n",
    "            lemma = \"nhs\"\n",
    "        elif wntag == \"\":\n",
    "            lemma = lemmatiser.lemmatize(token)\n",
    "        else:\n",
    "            lemma = lemmatiser.lemmatize(token, pos=wntag)\n",
    "        tok_text.append(lemma)\n",
    "\n",
    "    return tok_text\n",
    "\n",
    "def clean_text(comment):\n",
    "    \"\"\"takes a single comment as input\n",
    "    removes regular experession such as \"address removed\"\n",
    "    lowercases the words\n",
    "    removes punctuations, number and stopwords\n",
    "    corrects spelling\n",
    "    lemmatises word using wordnet\n",
    "    returns list of list of lemmatised words.\"\"\"\n",
    "    comments = []\n",
    "\n",
    "    row = wtc.reg_words(comment)\n",
    "    row = row.lower()\n",
    "\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "\n",
    "    words = row.split()\n",
    "    \n",
    "    comments = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    commentt = [sp.correct(word) if sp.correct(word.lower()) != word else word for word in comments]\n",
    "    cleaned_text = token_text_an(commentt)\n",
    "\n",
    "    return commentt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ce8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stopwords = []\n",
    "\n",
    "# comments1 = [reg_words(row) for row in datadf['comments_raw']]\n",
    "comments1 = [reg_words(row) for row in datadf['Comments']]\n",
    "\n",
    "for row in comments1:\n",
    "    filtered_sentence = [w for w in row.split(\" \") if not w.lower() in stop_words]\n",
    "    filtered_sentence = [sp.correct(word) if sp.correct(word.lower()) != word else word for word in filtered_sentence]\n",
    "\n",
    "    no_stopwords.append(\" \".join(filtered_sentence))\n",
    "\n",
    "datadf['stopwords_removed'] = no_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293be4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70edb66c",
   "metadata": {},
   "source": [
    "## Nouns and adjectives only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_text_an(comment):\n",
    "    \"\"\"input: list of comments\n",
    "    output: list of text lemmatised and tokenised\"\"\"\n",
    "    tok_text = []\n",
    "    token_tag = nltk.pos_tag(comment)  # tags words with POS tag\n",
    "\n",
    "    for token, tag in token_tag:\n",
    "#         print(token)\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "#         print (wntag)\n",
    "        if token == \"nhs\" or wntag in ['n', 'a']:\n",
    "            lemma = token\n",
    "        elif wntag != \"\":\n",
    "#             print(\"##wntag blank##\", token)\n",
    "            lemma = lemmatiser.lemmatize(token, pos=wntag)\n",
    "        else:\n",
    "            lemma = lemmatiser.lemmatize(token)        \n",
    "        tok_text.append(lemma)\n",
    "#             print(\"token printed ##\")\n",
    "       \n",
    "    return tok_text\n",
    "\n",
    "# corpus_plus = [\"nhs\", \"osteoporosis\", \"scolliosis\", \"leukaemia\"]\n",
    "\n",
    "def clean_text_an(comment):\n",
    "    \"\"\"takes a single comment as input\n",
    "    removes regular experession such as \"address removed\"\n",
    "    lowercases the words\n",
    "    removes punctuations, number and stopwords\n",
    "    corrects spelling\n",
    "    lemmatises word using wordnet\n",
    "    returns list of list of lemmatised words.\"\"\"\n",
    "    comments = []\n",
    "    spelled = []\n",
    "\n",
    "    row = wtc.reg_words(comment)\n",
    "    row = row.lower()\n",
    "\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "\n",
    "    words = row.split()\n",
    "    \n",
    "    comments = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    commentt = [sp.correct(word) if sp.correct(word.lower()) != word else word for word in comments]\n",
    "    cleaned_text = token_text_an(commentt)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [clean_text_an(row) for row in datadf['expanded_contractions']]\n",
    "datadf['nouns_adj'] = cleaned\n",
    "# cleaned\n",
    "datadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c48ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [clean_text(row) for row in datadf['expanded_contractions']]\n",
    "datadf['tokenised'] = cleaned\n",
    "# cleaned"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6d1e137997c1a626cf10baab9ffae7429987d9eccd9c4bad27c8c499ce74039"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
